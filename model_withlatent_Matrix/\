# -*- coding: utf-8 -*-
"""Embedding_version_0.3 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WQvTdH2Hvm-i7vYcYoICWtg4FXV5wFdS
"""

# Two steps solution
# Using Auto-Encoder to cast atoms-bonds images
# to a continuous embedding (-1.0, 1.0)
# Decoder for translating to SMILES

import warnings
warnings.filterwarnings('ignore')

import os
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import random
import numpy as np
from numpy import ndarray
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
import pickle

from tensorflow.keras.layers import (Input, Dropout, LSTM, Reshape, LeakyReLU,
                          Concatenate, ReLU, Flatten, Dense, Embedding,
                          BatchNormalization, Activation, SpatialDropout1D,
                          Conv2D, Conv1D, MaxPooling1D, MaxPooling2D, Softmax)
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.optimizers import Adam, RMSprop
import tensorflow.keras.backend as K
from tensorflow.keras.activations import tanh

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import np_utils
from tensorflow.keras.utils import to_categorical
from IPython.display import clear_output
import matplotlib.pyplot as plt

from progressbar import ProgressBar
import seaborn as sns

os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(42)
random.seed(12345)
#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)
#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options=gpu_options)
#tf.set_random_seed(1234)
#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
#K.set_session(sess)

with open('./../data/trainingsets/20000_train_regular_qm9/image_train.pickle', 'rb') as f:
    X_smiles_train, X_atoms_train, X_bonds_train, y_train = pickle.load(f)

with open('./../data/trainingsets/20000_train_regular_qm9/image_test.pickle', 'rb') as f:
    X_smiles_test, X_atoms_test, X_bonds_test, y_test = pickle.load(f)

# Subsampling has been done in the data preprocesses
X_smiles_train.shape, X_smiles_test.shape
"""
# Outlier removal
IQR = - np.quantile(y_train, 0.25) + np.quantile(y_train, 0.75)

lower_bound, upper_bound = np.quantile(y_train, 0.25) - 1.5 * IQR, np.quantile(y_train, 0.75) + 1.5 * IQR

idx = np.where((y_train >= lower_bound) & (y_train <= upper_bound))

y_train = y_train[idx]
X_smiles_train = X_smiles_train[idx]
X_atoms_train = X_atoms_train[idx]
X_bonds_train = X_bonds_train[idx]
"""
def norm(X: ndarray) -> ndarray:
    X = np.where(X == 0, -1.0, 1.0)
    return X

X_atoms_train, X_bonds_train = (norm(X_atoms_train),
                                norm(X_bonds_train))

X_atoms_test, X_bonds_test = (norm(X_atoms_test),
                              norm(X_bonds_test))

# plt the dist of training and testing set
sns.distplot(y_train, norm_hist = True)
plt.savefig('hc_train.png')
sns.distplot(y_test, norm_hist = True)
plt.savefig('hc_test.png')

# Normalize y_train, y_test
s_min1 = np.min (y_train)
s_max1 = np.max (y_train)

s_min2 = np.min(y_test)
s_max2 = np.max(y_test)
s_min = min(s_min1, s_min2)
s_max = max(s_max1, s_max2)
#s_min, s_max = 20, 50

y_train = (y_train - s_min) / (s_max - s_min)
y_test = (y_test - s_min) / (s_max - s_min)

print ("min and max train data and test normalized", s_min, s_max, np.min(y_test), np.max(y_test))
print ("min and max train data and train normalized", s_min, s_max, np.min(y_train), np.max(y_train))
# Encoding to an image embedding

# ENCODER
inp_1 = Input(shape = [9, 10, 1])
inp_2 = Input(shape = [9, 9, 4])

y1 = Conv2D(64, (2, 3), strides = 1, padding = 'valid')(inp_1)
y1 = LeakyReLU(alpha = 0.2)(y1)
y1 = BatchNormalization()(y1)

y1 = Conv2D(64, 2, strides = 1, padding = 'valid')(y1)
y1 = LeakyReLU(alpha = 0.2)(y1)
y1 = BatchNormalization()(y1)

y1 = Conv2D(64, 2, strides = 1, padding = 'valid')(y1)
y1 = LeakyReLU(alpha = 0.2)(y1)
y1 = BatchNormalization()(y1)

y1_emb = Conv2D(1, 1, strides = 1, padding = 'same',
            activation = 'tanh')(y1)

y2 = Conv2D(64, 2, strides = 1, padding = 'valid')(inp_2)
y2 = LeakyReLU(alpha = 0.2)(y2)
y2 = BatchNormalization()(y2)

y2 = Conv2D(64, 2, strides = 1, padding = 'valid')(y2)
y2 = LeakyReLU(alpha = 0.2)(y2)
y2 = BatchNormalization()(y2)

y2 = Conv2D(64, 2, strides = 1, padding = 'valid')(y2)
y2 = LeakyReLU(alpha = 0.2)(y2)
y2 = BatchNormalization()(y2)

y2_emb = Conv2D(1, 1, strides = 1, padding = 'same',
                activation = 'tanh')(y2)

####
y_out = Concatenate()([y1_emb, y2_emb])
y_out = Flatten()(y_out)

# DECODER
emb_in = Input(shape = (128,))

tower0 = Conv1D(32, 1, padding = 'same')(emb_in)
tower1 = Conv1D(64, 1, padding = 'same')(emb_in)
tower1 = Conv1D(64, 3, padding = 'same')(tower1)
tower2 = Conv1D(32, 1, padding = 'same')(emb_in)
tower2 = Conv1D(32, 5, padding = 'same')(tower2)
tower3 = MaxPooling2D(3, 1, padding = 'same')(emb_in)
tower3 = Conv1D(32, 1, padding = 'same')(tower3)
h = Concatenate()([tower0, tower1, tower2, tower3])
h = ReLU()(h)
h = MaxPooling1D(2, 1, padding = 'same')(h)

for i in range(6):
    tower0 = Conv1D(32, 1, padding = 'same')(h)
    tower1 = Conv1D(64, 1, padding = 'same')(h)
    tower1 = Conv1D(64, 3, padding = 'same')(tower1)
    tower2 = Conv1D(32, 1, padding = 'same')(h)
    tower2 = Conv1D(32, 5, padding = 'same')(tower2)
    tower3 = MaxPooling2D(3, 1, padding = 'same')(h)
    tower3 = Conv1D(32, 1, padding = 'same')(tower3)
    h = Concatenate()([tower0, tower1, tower2, tower3])
    h = ReLU()(h)
    if i % 2 == 0 and i != 0:
        h = MaxPooling1D(2, 1, padding = 'same')(h)
h = BatchNormalization()(h)

y = Flatten()(h)

y = Dense(256, activation = 'relu')(y)
y_cv = Dense(64, activation = 'relu')(y)
y = Dropout(0.2)(y)
y = Dense(128, activation = 'relu')(y)
y = Dropout(0.2)(y)
y = Dense(128, activation = 'relu')(y)
y = Dropout(0.2)(y)
y = Dense(35 * 23)(y)
y = Reshape([35, 23, 1])(y)
y = Softmax(axis = 2)(y)

y_cv = Dropout(0.2)(y_cv)
y_cv = Dense(128, activation = 'relu')(y_cv)
y_cv = Dropout(0.2)(y_cv)
y_cv = Dense(128, activation = 'relu')(y_cv)
y_cv = Dense(1, activation = 'sigmoid')(y_cv)

encoder = Model([inp_1, inp_2], [y1_emb, y2_emb, y_out], name = 'Encoder')
decoder = Model(emb_in, [y, y_cv], name = 'Decoder')
print (encoder.summary())
print (decoder.summary())
outputs = decoder(encoder([inp_1, inp_2])[2])
model = Model([inp_1, inp_2], outputs, name = 'ae')
"""
lr_schedule = keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10 ** (epoch / 20)
)

encoder = load_model('encoder.h5')
decoder = load_model('decoder.h5')
model = load_model('ae_model.h5')

model.compile(optimizer = Adam(learning_rate = 1e-8),
              loss = ['binary_crossentropy', 'mse'])
history = model.fit([X_atoms_train, X_bonds_train],
                    [X_smiles_train, y_train],
                    validation_data = ([X_atoms_test, X_bonds_test],
                                       [X_smiles_test, y_test]),
                    epochs = 1,
                    batch_size = 32,
                    verbose = 1,
                    callbacks = [lr_schedule])



plt.semilogx(history.history['lr'],
             history.history['val_Decoder_loss'])

encoder = Model([inp_1, inp_2], [y1_emb, y2_emb, y_out], name = 'Encoder')
decoder = Model(emb_in, [y, y_cv], name = 'Decoder')

print (encoder.summary())
print (decoder.summary())

outputs = decoder(encoder([inp_1, inp_2])[2])
model = Model([inp_1, inp_2], outputs, name = 'ae')

model.compile(optimizer = Adam(learning_rate = 9e-5),
              loss = ['binary_crossentropy', 'mse'])

model.fit([X_atoms_train, X_bonds_train],
                    [X_smiles_train, y_train],
                    validation_data = ([X_atoms_test, X_bonds_test],
                                       [X_smiles_test, y_test]),
                    epochs = 1,
                    batch_size = 32,
                    verbose = 1)
"""
#encoder = load_model('./../data/nns_latentonly/encoder.h5')
#decoder = load_model('./../data/nns_latentonly/decoder.h5')
#model = load_model  ('./../data/nns_latentonly/ae_model.h5')
"""
model.compile(optimizer = Adam(learning_rate = 9e-5),
              loss = ['binary_crossentropy', 'mse'])
"""

model.fit([X_atoms_train, X_bonds_train],
                    [X_smiles_train, y_train],
                    validation_data = ([X_atoms_test, X_bonds_test],
                                       [X_smiles_test, y_test]),
                    epochs = 1,
                    batch_size = 32,
                    verbose = 1)

for i in [5, 10, 32, 88, 99]:
    plt.subplot(121)
    plt.imshow(X_smiles_train[i].reshape([35, 23]))
    test_sample_pred = decoder.predict(encoder.predict([X_atoms_train[i:(i+2)], X_bonds_train[i:(i+2)]])[2])[0][0]
    plt.subplot(122)
    plt.imshow(test_sample_pred.reshape([35, 23]))
    plt.show()
    plt.savefig("smiles_{}_train.png".format(i)) 

output = decoder.predict(encoder.predict([X_atoms_train[5:7], X_bonds_train[5:7]])[2])
print (output[0][0].shape)
print(y_train[5])

for i in [5, 10, 32, 88, 99]:
    plt.subplot(121)
    plt.imshow(X_smiles_test[i].reshape([35, 23]))
    test_sample_pred = decoder.predict(encoder.predict([X_atoms_test[i:(i+2)], X_bonds_test[i:(i+2)]])[2])[0][0]
    plt.subplot(122)
    plt.imshow(test_sample_pred.reshape([35, 23]))
    plt.show()
    plt.savefig("smiles_{}_test.png".format(i))

model.save  ('./../data/nns_latentonly/ae_model.h5')
encoder.save('./../data/nns_latentonly/encoder.h5')
decoder.save('./../data/nns_latentonly/decoder.h5')
